{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-2 : text generation",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oy2mBkB2l41"
      },
      "source": [
        "# Text generation: GPT-2\n",
        "Conditional text generation with the auto-regressive models of the library (GPT/GPT-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ40onIZUpPL"
      },
      "source": [
        "## License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDSsr-CdI7eo"
      },
      "source": [
        "```\n",
        "# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYIlnnpKUuvg"
      },
      "source": [
        "## Let's get started !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6Iwpq0F2hRn"
      },
      "source": [
        "# coding=utf-8\n",
        "\n",
        "import logging\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "!pip install transformers\n",
        "\n",
        "from transformers import GPT2Config\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaAgjHNEU7f1"
      },
      "source": [
        "Padding text to help Transformer-XL and XLNet with short prompts as proposed by Aman Rusia in https://github.com/rusiaaman/XLNet-gen#methodology and https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlbHEmFZU7xq"
      },
      "source": [
        "PADDING_TEXT = \"\"\" In 1991, the remains of Russian Tsar Nicholas II and his family\n",
        "(except for Alexei and Maria) are discovered.\n",
        "The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\n",
        "remainder of the story. 1883 Western Siberia,\n",
        "a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
        "Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
        "father initially slaps him for making such an accusation, Rasputin watches as the\n",
        "man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
        "the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
        "with people, even a bishop, begging for his blessing. <eod> </s> <eos>\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7ciVoTTVr1M"
      },
      "source": [
        "## Helper functions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JACOGhUDVrlP"
      },
      "source": [
        "def set_seed(seed, n_gpu):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01Udu-MTVxZL"
      },
      "source": [
        "The function `top_k_top_p_filtering` does the following:\n",
        "\n",
        "Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        \n",
        "Args:\n",
        "* logits: logits distribution shape (batch size x vocabulary size)\n",
        "* top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
        "* top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "                \n",
        "Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziPK1bIEWE3W"
      },
      "source": [
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" \n",
        "    \"\"\"\n",
        "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # scatter sorted tensors to original indexing\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOgMQ7jUWHV1"
      },
      "source": [
        "## function `sample_sequence`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PWm0t2SWHqu"
      },
      "source": [
        "def sample_sequence(model, length, context, num_samples=1, temperature=1, top_k=0, top_p=0.0, repetition_penalty=1.0,\n",
        "                    device='cpu'):\n",
        "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
        "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
        "    generated = context\n",
        "    with torch.no_grad():\n",
        "        for _ in trange(length):\n",
        "\n",
        "            inputs = {'input_ids': generated}\n",
        "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2 (cached hidden-states)\n",
        "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\n",
        "\n",
        "            # repetition penalty from CTRL (https://arxiv.org/abs/1909.05858)\n",
        "            for i in range(num_samples):\n",
        "                for _ in set(generated[i].tolist()):\n",
        "                    next_token_logits[i, _] /= repetition_penalty\n",
        "                \n",
        "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "            if temperature == 0: # greedy sampling:\n",
        "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\n",
        "            else:\n",
        "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
        "            generated = torch.cat((generated, next_token), dim=1)\n",
        "    return generated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1g_7NwgWPQM"
      },
      "source": [
        "# Main function: `generate_text`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdifIMXu2vok"
      },
      "source": [
        "def generate_text(raw_text, \n",
        "                  prompt=\"\", \n",
        "                  padding_text=\"\", \n",
        "                  words=50, \n",
        "                  num_samples=1, \n",
        "                  temperature=1.0, \n",
        "                  repetition_penalty=1.0, \n",
        "                  top_k=0, \n",
        "                  top_p=0.9, \n",
        "                  no_cuda=False, \n",
        "                  seed=42, \n",
        "                  model_path=\"gpt2-medium\"):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "\n",
        "    set_seed(1337, n_gpu)\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    context_tokens = tokenizer.encode(raw_text, add_special_tokens=False)\n",
        "    out = sample_sequence(\n",
        "        model=model,\n",
        "        context=context_tokens,\n",
        "        num_samples=num_samples,\n",
        "        length=words,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        device=device,\n",
        "    )\n",
        "    out = out[:, len(context_tokens):].tolist()\n",
        "    print(\"\\n\"+ raw_text)\n",
        "    for o in out:\n",
        "        text = tokenizer.decode(o, clean_up_tokenization_spaces=True)\n",
        "    print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10___sCC21HU"
      },
      "source": [
        "generate_text(\"slowly we rounded the corner, exclaiming to the dragon that we could if only we wanted \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq9LSSsnih1t"
      },
      "source": [
        "# Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDT-pCPJijsY"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler                                                                                                                                                       \n",
        "\n",
        "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,                                                           \n",
        "                                  GPT2Config, GPT2LMHeadModel, GPT2Tokenizer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DoqHE5qjb5X"
      },
      "source": [
        "First we'll define our own data loader in order to import some of our own text to finetune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f03rkgrOjcC8"
      },
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, file_path, block_size):\n",
        "        assert os.path.isfile(file_path)\n",
        "        directory, filename = os.path.split(file_path)\n",
        "        cached_features_file = os.path.join(directory, 'cached_lm_' + str(block_size) + '_' + filename)\n",
        "\n",
        "        if os.path.exists(cached_features_file):\n",
        "            print(\"Loading features from cached file %s\", cached_features_file)\n",
        "            with open(cached_features_file, 'rb') as handle:\n",
        "                self.examples = pickle.load(handle)\n",
        "        else:\n",
        "            print(\"Creating features from dataset file at %s\", directory)\n",
        "\n",
        "            self.examples = []\n",
        "            with open(file_path, encoding=\"utf-8\") as f:\n",
        "                text = f.read()\n",
        "\n",
        "            tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
        "\n",
        "            for i in range(0, len(tokenized_text)-block_size+1, block_size): # Truncate in block of block_size\n",
        "                self.examples.append(tokenizer.build_inputs_with_special_tokens(tokenized_text[i:i+block_size]))\n",
        "            # Note that we are loosing the last truncated example here for the sake of simplicity (no padding)\n",
        "            # If your dataset is small, first you should loook for a bigger one :-) and second you\n",
        "            # can change this behavior by adding (model specific) padding.\n",
        "\n",
        "            print(\"Saving features into cached file %s\", cached_features_file)\n",
        "            with open(cached_features_file, 'wb') as handle:\n",
        "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return torch.tensor(self.examples[item])\n",
        "\n",
        "\n",
        "def load_and_cache_examples(train_data_file, tokenizer, block_size):\n",
        "    print(\"Loading dataset %s\", train_data_file)\n",
        "    dataset = TextDataset(tokenizer, file_path=train_data_file, block_size=block_size)\n",
        "    return dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SIZwPLyO2GG"
      },
      "source": [
        "## Training Routine\n",
        "This is the code responsible for doing each of the small incremental updates to the model based on the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYbbMxwqkWg8"
      },
      "source": [
        "def train(train_dataset, model, tokenizer, batch_size, max_datapoints, gradient_accumulation_steps, learning_rate, \n",
        "          weight_decay, adam_epsilon, max_grad_norm, num_train_epochs, warmup_steps, n_gpu, device):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    train_batch_size = batch_size * max(1, n_gpu)\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size)\n",
        "\n",
        "    t_total = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
        "\n",
        "    # multi-gpu training\n",
        "    if n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Only train the head of the model for improved mem and speed\n",
        "    for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    for param in model.lm_head.parameters():\n",
        "      param.requires_grad = True\n",
        "\n",
        "    print(\"Num examples = \", len(train_dataset), \"\\nNum Epochs = \", num_train_epochs, \n",
        "        \"\\nInstantaneous batch size per GPU = \", batch_size, \n",
        "        \"\\nTotal train batch size (w. parallel & accumulation) = \", train_batch_size * gradient_accumulation_steps ,\n",
        "        \"\\nGradient Accumulation steps = \", gradient_accumulation_steps,\n",
        "        \"\\nTotal optimization steps = \", t_total)\n",
        "\n",
        "    global_step = 0\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    train_iterator = trange(int(num_train_epochs), desc=\"Epoch\")\n",
        "    set_seed(1337, n_gpu)  # Added here for reproducibility (even between python 2 and 3)\n",
        "    model.train()\n",
        "    for _ in train_iterator:\n",
        "        epoch_iterator = tqdm(list(zip(range(max_datapoints), train_dataloader)), desc=\"Iteration\", position=0, leave=True)\n",
        "        for step, batch in epoch_iterator:\n",
        "            inputs, labels = (batch, batch)\n",
        "            if str(device) != \"cpu\":\n",
        "              inputs = inputs.to(device)\n",
        "              labels = labels.to(device)\n",
        "            outputs = model(inputs, labels=labels)\n",
        "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
        "\n",
        "            if n_gpu > 1:\n",
        "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
        "            if gradient_accumulation_steps > 1:\n",
        "                loss = loss / gradient_accumulation_steps\n",
        "            \n",
        "            loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "    return global_step, tr_loss / global_step\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPAEG9BWkuog"
      },
      "source": [
        "## Run the finetuning\n",
        "This is our main method, like above, but there are two steps, the first being to run the finetuning, and the second to generate the text from that fine-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAzEzRLck-eV"
      },
      "source": [
        "def finetune(train_data_file, output_dir, model_path=\"gpt2-medium\", block_size=512, do_lower_case=True,\n",
        "             batch_size=1, max_datapoints=200, gradient_accumulation_steps=5, learning_rate=3e-4, \n",
        "             weight_decay=0.0, adam_epsilon=1e-8, max_grad_norm=1.0, num_train_epochs=1.0, warmup_steps=0):\n",
        "\n",
        "    if os.path.exists(output_dir) and os.listdir(output_dir):\n",
        "        print(\"WARNING: Output directory ({}) already exists and is not empty. Overwriting.\".format(output_dir))\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "    device = device\n",
        "    print(\"device: %s, n_gpu: %s\", device, n_gpu)\n",
        "    set_seed(42, n_gpu)\n",
        "\n",
        "    config = GPT2Config.from_pretrained(model_path)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_path, do_lower_case=do_lower_case)\n",
        "    block_size = min(block_size, tokenizer.max_len_single_sentence)\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_path, config=config)\n",
        "    model.to(device)\n",
        "\n",
        "    train_dataset = load_and_cache_examples(train_data_file, tokenizer, block_size)\n",
        "    global_step, tr_loss = train(train_dataset, model, tokenizer, batch_size, max_datapoints, gradient_accumulation_steps, learning_rate, \n",
        "                                 weight_decay, adam_epsilon, max_grad_norm, num_train_epochs, warmup_steps, n_gpu, device)\n",
        "    print(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
        "\n",
        "    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
        "    # Create output directory if needed\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    print(\"Saving model checkpoint to %s\", output_dir)\n",
        "    # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "    # They can then be reloaded using `from_pretrained()`\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model\n",
        "    model_to_save.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XuKpNz8Hday"
      },
      "source": [
        "finetune(train_data_file=\"tiny-shakespeare.txt\", output_dir=\"./output\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmoUk6JpGV75"
      },
      "source": [
        "generate_text(\"Why, man, he doth bestride the narrow world \", model_path=\"./output\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}