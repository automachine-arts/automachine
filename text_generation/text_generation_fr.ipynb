{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-2 : text generation - français",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oy2mBkB2l41"
      },
      "source": [
        "# Génération de texte avec GPT-2\n",
        "\n",
        "Génération conditionnelle de texte à l'aide d'un modèle auto-regressif de la librairie GPT/GPT2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ40onIZUpPL"
      },
      "source": [
        "## Licence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vh3-M-qKUpmp"
      },
      "source": [
        "# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYIlnnpKUuvg"
      },
      "source": [
        "## Commençons !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6Iwpq0F2hRn"
      },
      "source": [
        "# coding=utf-8\n",
        "\n",
        "import logging\n",
        "from tqdm import trange\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "\n",
        "!pip install transformers\n",
        "\n",
        "from transformers import GPT2Config\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaAgjHNEU7f1"
      },
      "source": [
        "Ajoutons du texte à l'invitation (*prompt*) qui sera faite plus tard afin d'aider Transformer-XL et XLNet lorsque l'invitation reçue est courte, tel que mentionné par Aman Rusia dans https://github.com/rusiaaman/XLNet-gen#methodology et dans https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlbHEmFZU7xq"
      },
      "source": [
        "PADDING_TEXT = \"\"\" In 1991, the remains of Russian Tsar Nicholas II and his family\n",
        "(except for Alexei and Maria) are discovered.\n",
        "The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\n",
        "remainder of the story. 1883 Western Siberia,\n",
        "a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
        "Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
        "father initially slaps him for making such an accusation, Rasputin watches as the\n",
        "man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
        "the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
        "with people, even a bishop, begging for his blessing. <eod> </s> <eos>\"\"\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7ciVoTTVr1M"
      },
      "source": [
        "## Fonctions adjacentes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JACOGhUDVrlP"
      },
      "source": [
        "def set_seed(seed, n_gpu):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(seed)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01Udu-MTVxZL"
      },
      "source": [
        "La fonction `top_k_top_p_filtering` permet de fitlrer une distribution de *logits* en utilisant le filtrage top-k ou le filtrage *nucleus* top-p.\n",
        "\n",
        "Arguments:\n",
        "* *logits*: forme de la distribution des logits (taille de l'essai en cours (*batch size*) multipliée par la taille du vocabulaire)\n",
        "* top_k > 0 :  garder uniquement les top k jetons ayant la plus forte probabilité (filtrage top-k)\n",
        "* top_p > 0.0 : garder uniquement les top k jetons ayant une probabilité cumulative supérieure à top_p (filtrage par *nucleus*)\n",
        "\n",
        "Le filtrage par *nucleus* est expliqué dans Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "\n",
        "Pris dans: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziPK1bIEWE3W"
      },
      "source": [
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" \n",
        "    \"\"\"\n",
        "    top_k = min(top_k, logits.size(-1))  # Mesure de sécurité\n",
        "    if top_k > 0:\n",
        "        # Enlève tous les jetons ayant une probabilité inférieure au dernier jeton du top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "        # Enlève les jetons avec une probabilité cumulative supérieure au seuil.\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Déplace les jetons vers la droite afin d'inclure le premier jeton au-dessus du seuil.\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # Déplace les tenseurs ainsi classés selon l'indice original.\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOgMQ7jUWHV1"
      },
      "source": [
        "## fonction `sample_sequence`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PWm0t2SWHqu"
      },
      "source": [
        "def sample_sequence(model, length, context, num_samples=1, temperature=1, top_k=0, top_p=0.0, repetition_penalty=1.0,\n",
        "                    device='cpu'):\n",
        "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
        "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
        "    generated = context\n",
        "    with torch.no_grad():\n",
        "        for _ in trange(length):\n",
        "\n",
        "            inputs = {'input_ids': generated}\n",
        "            outputs = model(**inputs) # Note: on peut aussi utiliser 'past' avec GPT-2 (états cachés dans le cache)\n",
        "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\n",
        "\n",
        "            # pénalité de répétition vient de CTRL (https://arxiv.org/abs/1909.05858)\n",
        "            for i in range(num_samples):\n",
        "                for _ in set(generated[i].tolist()):\n",
        "                    next_token_logits[i, _] /= repetition_penalty\n",
        "                \n",
        "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "            if temperature == 0: # échantillonnage avare:\n",
        "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\n",
        "            else:\n",
        "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
        "            generated = torch.cat((generated, next_token), dim=1)\n",
        "    return generated"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1g_7NwgWPQM"
      },
      "source": [
        "# Main function: `generate_text`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdifIMXu2vok"
      },
      "source": [
        "def generate_text(raw_text, \n",
        "                  prompt=\"\", \n",
        "                  padding_text=\"\", \n",
        "                  words=50, \n",
        "                  num_samples=1, \n",
        "                  temperature=1.0, \n",
        "                  repetition_penalty=1.0, \n",
        "                  top_k=0, \n",
        "                  top_p=0.9, \n",
        "                  no_cuda=False, \n",
        "                  seed=42, \n",
        "                  model_path=\"gpt2-medium\"):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "\n",
        "    set_seed(1337, n_gpu)\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    context_tokens = tokenizer.encode(raw_text, add_special_tokens=False)\n",
        "    out = sample_sequence(\n",
        "        model=model,\n",
        "        context=context_tokens,\n",
        "        num_samples=num_samples,\n",
        "        length=words,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        device=device,\n",
        "    )\n",
        "    out = out[:, len(context_tokens):].tolist()\n",
        "    print(\"\\n\"+ raw_text)\n",
        "    for o in out:\n",
        "        text = tokenizer.decode(o, clean_up_tokenization_spaces=True)\n",
        "    print(text)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10___sCC21HU"
      },
      "source": [
        "generate_text(\"slowly we rounded the corner, exclaiming to the dragon that we could if only we wanted \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq9LSSsnih1t"
      },
      "source": [
        "# Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDT-pCPJijsY"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler                                                                                                                                                       \n",
        "\n",
        "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,                                                           \n",
        "                                  GPT2Config, GPT2LMHeadModel, GPT2Tokenizer)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DoqHE5qjb5X"
      },
      "source": [
        "Premièrement, nous allons définir notre propre fonction d'importation de données afin de pouvoir utiliser des textes de notre choix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f03rkgrOjcC8"
      },
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, file_path, block_size):\n",
        "        assert os.path.isfile(file_path)\n",
        "        directory, filename = os.path.split(file_path)\n",
        "        cached_features_file = os.path.join(directory, 'cached_lm_' + str(block_size) + '_' + filename)\n",
        "\n",
        "        if os.path.exists(cached_features_file):\n",
        "            print(\"Loading features from cached file %s\", cached_features_file)\n",
        "            with open(cached_features_file, 'rb') as handle:\n",
        "                self.examples = pickle.load(handle)\n",
        "        else:\n",
        "            print(\"Creating features from dataset file at %s\", directory)\n",
        "\n",
        "            self.examples = []\n",
        "            with open(file_path, encoding=\"utf-8\") as f:\n",
        "                text = f.read()\n",
        "\n",
        "            tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
        "\n",
        "            for i in range(0, len(tokenized_text)-block_size+1, block_size): # Découpe en blocs d'une grandeur block_size\n",
        "                self.examples.append(tokenizer.build_inputs_with_special_tokens(tokenized_text[i:i+block_size]))\n",
        "            # Ici, nous perdons le dernier exemple découpé par soucis de simplicité (pas de padding)\n",
        "            # Si votre jeu de données est petit, commencez par tenter d'en trouver un plus grand :)\n",
        "            # et ensuite, vous pouvez changer ce comportement en ajoutant du padding spécifique au modèle.\n",
        "\n",
        "            print(\"Saving features into cached file %s\", cached_features_file)\n",
        "            with open(cached_features_file, 'wb') as handle:\n",
        "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return torch.tensor(self.examples[item])\n",
        "\n",
        "\n",
        "def load_and_cache_examples(train_data_file, tokenizer, block_size):\n",
        "    print(\"Loading dataset %s\", train_data_file)\n",
        "    dataset = TextDataset(tokenizer, file_path=train_data_file, block_size=block_size)\n",
        "    return dataset\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SIZwPLyO2GG"
      },
      "source": [
        "## Routine d'apprentissage\n",
        "\n",
        "Cette partie est responsable de faire chacune des petites mises-à-jour du modèle qui se base sur le jeu de données."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYbbMxwqkWg8"
      },
      "source": [
        "def train(train_dataset, model, tokenizer, batch_size, max_datapoints, gradient_accumulation_steps, learning_rate, \n",
        "          weight_decay, adam_epsilon, max_grad_norm, num_train_epochs, warmup_steps, n_gpu, device):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    train_batch_size = batch_size * max(1, n_gpu)\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size)\n",
        "\n",
        "    t_total = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
        "\n",
        "    # apprentissage sur de multiples gpus\n",
        "    if n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # afin d'améliorer la mémoire et la vitesse, réaliser l'apprentissage uniquement sur la tête du modèle.\n",
        "    for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    for param in model.lm_head.parameters():\n",
        "      param.requires_grad = True\n",
        "\n",
        "    print(\"Num examples = \", len(train_dataset), \"\\nNum Epochs = \", num_train_epochs, \n",
        "        \"\\nInstantaneous batch size per GPU = \", batch_size, \n",
        "        \"\\nTotal train batch size (w. parallel & accumulation) = \", train_batch_size * gradient_accumulation_steps ,\n",
        "        \"\\nGradient Accumulation steps = \", gradient_accumulation_steps,\n",
        "        \"\\nTotal optimization steps = \", t_total)\n",
        "\n",
        "    global_step = 0\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    train_iterator = trange(int(num_train_epochs), desc=\"Epoch\")\n",
        "    set_seed(1337, n_gpu)  # ajouter ici pour pouvoir reproduire le processus (même entre python2 et 3)\n",
        "    model.train()\n",
        "    for _ in train_iterator:\n",
        "        epoch_iterator = tqdm(list(zip(range(max_datapoints), train_dataloader)), desc=\"Iteration\", position=0, leave=True)\n",
        "        for step, batch in epoch_iterator:\n",
        "            inputs, labels = (batch, batch)\n",
        "            if str(device) != \"cpu\":\n",
        "              inputs = inputs.to(device)\n",
        "              labels = labels.to(device)\n",
        "            outputs = model(inputs, labels=labels)\n",
        "            loss = outputs[0]  # les sorties d'un modèle sont toujours des tuples dans transformer (voir la doc)\n",
        "\n",
        "            if n_gpu > 1:\n",
        "                loss = loss.mean()  # mean() afin de calculer la moyenne lorsqu'on utilise plusieurs gpus\n",
        "            if gradient_accumulation_steps > 1:\n",
        "                loss = loss / gradient_accumulation_steps\n",
        "            \n",
        "            loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "    return global_step, tr_loss / global_step\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPAEG9BWkuog"
      },
      "source": [
        "## Run the finetuning\n",
        "This is our main method, like above, but there are two steps, the first being to run the finetuning, and the second to generate the text from that fine-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAzEzRLck-eV"
      },
      "source": [
        "def finetune(train_data_file, output_dir, model_path=\"gpt2-medium\", block_size=512, do_lower_case=True,\n",
        "             batch_size=1, max_datapoints=200, gradient_accumulation_steps=5, learning_rate=3e-4, \n",
        "             weight_decay=0.0, adam_epsilon=1e-8, max_grad_norm=1.0, num_train_epochs=1.0, warmup_steps=0):\n",
        "\n",
        "    if os.path.exists(output_dir) and os.listdir(output_dir):\n",
        "        print(\"WARNING: Output directory ({}) already exists and is not empty. Overwriting.\".format(output_dir))\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "    device = device\n",
        "    print(\"device: %s, n_gpu: %s\", device, n_gpu)\n",
        "    set_seed(42, n_gpu)\n",
        "\n",
        "    config = GPT2Config.from_pretrained(model_path)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_path, do_lower_case=do_lower_case)\n",
        "    block_size = min(block_size, tokenizer.max_len_single_sentence)\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_path, config=config)\n",
        "    model.to(device)\n",
        "\n",
        "    train_dataset = load_and_cache_examples(train_data_file, tokenizer, block_size)\n",
        "    global_step, tr_loss = train(train_dataset, model, tokenizer, batch_size, max_datapoints, gradient_accumulation_steps, learning_rate, \n",
        "                                 weight_decay, adam_epsilon, max_grad_norm, num_train_epochs, warmup_steps, n_gpu, device)\n",
        "    print(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
        "\n",
        "    # Pratiques préférées: si vous utilisez save_pretained pour le modèle et le\n",
        "    # tokenizer, vous pouvez les recharger en utiliser pretrained()\n",
        "    # Crée un répertoire de sortie si nécessaire\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    print(\"Saving model checkpoint to %s\", output_dir)\n",
        "    # sauvegarde un modèle entraîné, sa configuration et le tokenizer avec\n",
        "    # `save_pretrained()`.\n",
        "    # On peut les retrouver avec `from_pretrained()`\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model\n",
        "    model_to_save.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XuKpNz8Hday"
      },
      "source": [
        "finetune(train_data_file=\"tiny-shakespeare.txt\", output_dir=\"./output\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmoUk6JpGV75"
      },
      "source": [
        "generate_text(\"Why, man, he doth bestride the narrow world \", model_path=\"./output\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}